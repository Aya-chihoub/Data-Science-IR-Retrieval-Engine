{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Science Project: Information Retrieval Engine\n",
        "## Phase 1: Retrieval Basics\n",
        "\n",
        "**Team Members:** *(To be added)*\n",
        "\n",
        "**Phase 1 Deadline:** March 2, 2026, 11:59 PM\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. [Setup & Imports](#1-setup--imports)\n",
        "2. [Data Loading & Preparation](#2-data-loading--preparation)\n",
        "3. [Data Analysis & Statistics](#3-data-analysis--statistics)\n",
        "4. [Retrieval Methods](#4-retrieval-methods)\n",
        "   - 4.1 TF-IDF Retrieval\n",
        "   - 4.2 BM25+ Retrieval\n",
        "   - 4.3 Text Embeddings (SentenceTransformers)\n",
        "5. [Visualization](#5-visualization)\n",
        "6. [Evaluation Suite](#6-evaluation-suite)\n",
        "7. [Results Comparison](#7-results-comparison)\n",
        "8. [Kaggle Submission](#8-kaggle-submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn rank_bm25 sentence-transformers umap-learn matplotlib seaborn tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "# Retrieval methods\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Plus\n",
        "\n",
        "# Embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "try:\n",
        "    import umap\n",
        "    UMAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    UMAP_AVAILABLE = False\n",
        "\n",
        "# Progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Settings\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Data Loading & Preparation\n",
        "\n",
        "The dataset consists of:\n",
        "- **Documents (docs.json)**: The document collection with id, title, text, tags, and category\n",
        "- **Training Queries (queries_train.json)**: Queries for evaluation\n",
        "- **Test Queries (queries_test.json)**: Queries for Kaggle submission\n",
        "- **Ground Truth (qgts_train.json)**: Relevant document IDs for each training query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# IMPORTANT: Download data from Kaggle first!\n",
        "# ============================================================\n",
        "# 1. Go to: https://www.kaggle.com/t/f383bc6c1f194226bb43d21ab3d65418\n",
        "# 2. Download and extract the dataset files to the 'data/' folder:\n",
        "#    - docs.json\n",
        "#    - queries_train.json  \n",
        "#    - queries_test.json\n",
        "#    - qgts_train.json\n",
        "# ============================================================\n",
        "\n",
        "DATA_DIR = Path('data/retrieval-engine-competition')\n",
        "\n",
        "# Check if data files exist\n",
        "required_files = ['docs.json', 'queries_train.json', 'queries_test.json', 'qgts_train.json']\n",
        "missing = [f for f in required_files if not (DATA_DIR / f).exists()]\n",
        "\n",
        "if missing:\n",
        "    print(\"‚ùå Missing data files in 'data/' folder:\")\n",
        "    for f in missing:\n",
        "        print(f\"   - {f}\")\n",
        "    print(\"\\nüì• Please download from Kaggle:\")\n",
        "    print(\"   https://www.kaggle.com/t/f383bc6c1f194226bb43d21ab3d65418\")\n",
        "else:\n",
        "    # Load the data\n",
        "    df_docs = pd.read_json(DATA_DIR / 'docs.json')\n",
        "    df_queries_train = pd.read_json(DATA_DIR / 'queries_train.json')\n",
        "    df_queries_test = pd.read_json(DATA_DIR / 'queries_test.json')\n",
        "\n",
        "    with open(DATA_DIR / 'qgts_train.json', 'r') as f:\n",
        "        ground_truth = json.load(f)\n",
        "\n",
        "    print(f\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"   Documents: {len(df_docs):,}\")\n",
        "    print(f\"   Training Queries: {len(df_queries_train):,}\")\n",
        "    print(f\"   Test Queries: {len(df_queries_test):,}\")\n",
        "    print(f\"   Ground Truth entries: {len(ground_truth):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the data\n",
        "print(\"üìÑ Sample Document:\")\n",
        "display(df_docs.head(2))\n",
        "\n",
        "print(\"\\n‚ùì Sample Query:\")\n",
        "display(df_queries_train.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Text Preprocessing\n",
        "\n",
        "We create a unified `content` field by merging title, text, and tags. This gives the retrieval models more context to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_fields(row):\n",
        "    \"\"\"Combine title, text, and tags into a single content field.\"\"\"\n",
        "    title = str(row.get('title', '') or '')\n",
        "    text = str(row.get('text', '') or '')\n",
        "    tags_list = row.get('tags', [])\n",
        "    tags = \" \".join(tags_list) if isinstance(tags_list, list) else \"\"\n",
        "    \n",
        "    combined = f\"{title} {text} {tags}\"\n",
        "    return \" \".join(combined.split())  # Normalize whitespace\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text: lowercase and remove punctuation.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"Processing documents...\")\n",
        "df_docs['content'] = df_docs.apply(merge_fields, axis=1)\n",
        "df_docs['content_clean'] = df_docs['content'].apply(clean_text)\n",
        "\n",
        "print(\"Processing training queries...\")\n",
        "df_queries_train['content'] = df_queries_train.apply(merge_fields, axis=1)\n",
        "df_queries_train['content_clean'] = df_queries_train['content'].apply(clean_text)\n",
        "\n",
        "print(\"Processing test queries...\")\n",
        "df_queries_test['content'] = df_queries_test.apply(merge_fields, axis=1)\n",
        "df_queries_test['content_clean'] = df_queries_test['content'].apply(clean_text)\n",
        "\n",
        "print(\"‚úÖ Text preprocessing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data Analysis & Statistics\n",
        "\n",
        "Before building our retrieval system, let's understand our dataset better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"=\"*60)\n",
        "print(\"üìä DATASET STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìÑ DOCUMENTS:\")\n",
        "print(f\"   Total documents: {len(df_docs):,}\")\n",
        "print(f\"   Columns: {list(df_docs.columns)}\")\n",
        "\n",
        "print(f\"\\n‚ùì QUERIES:\")\n",
        "print(f\"   Training queries: {len(df_queries_train):,}\")\n",
        "print(f\"   Test queries: {len(df_queries_test):,}\")\n",
        "\n",
        "print(f\"\\nüéØ RELEVANCE JUDGMENTS:\")\n",
        "total_relevant = sum(len(v) for v in ground_truth.values())\n",
        "avg_relevant = total_relevant / len(ground_truth)\n",
        "print(f\"   Total relevance judgments: {total_relevant:,}\")\n",
        "print(f\"   Average relevant docs per query: {avg_relevant:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Document length statistics\n",
        "df_docs['content_length'] = df_docs['content'].str.split().str.len()\n",
        "df_queries_train['content_length'] = df_queries_train['content'].str.split().str.len()\n",
        "\n",
        "print(\"\\nüìè CONTENT LENGTH STATISTICS:\")\n",
        "print(\"\\nDocuments:\")\n",
        "print(df_docs['content_length'].describe())\n",
        "\n",
        "print(\"\\nQueries:\")\n",
        "print(df_queries_train['content_length'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Category distribution (if available)\n",
        "if 'category' in df_docs.columns:\n",
        "    print(\"\\nüìÇ CATEGORY DISTRIBUTION:\")\n",
        "    category_counts = df_docs['category'].value_counts()\n",
        "    print(category_counts)\n",
        "    \n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    category_counts.plot(kind='bar', ax=ax, color='steelblue')\n",
        "    ax.set_title('Document Category Distribution')\n",
        "    ax.set_xlabel('Category')\n",
        "    ax.set_ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reports/category_distribution.png', dpi=150)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Retrieval Methods\n",
        "\n",
        "We implement three retrieval methods:\n",
        "1. **TF-IDF** - Term Frequency-Inverse Document Frequency\n",
        "2. **BM25+** - Best Matching 25 Plus\n",
        "3. **Text Embeddings** - Using SentenceTransformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 TF-IDF Retrieval\n",
        "\n",
        "**How TF-IDF works:**\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure that evaluates how important a word is to a document in a collection.\n",
        "\n",
        "- **TF (Term Frequency)**: How often a word appears in a document. More occurrences = higher importance.\n",
        "- **IDF (Inverse Document Frequency)**: How rare a word is across all documents. Rare words get higher scores.\n",
        "\n",
        "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\log\\left(\\frac{N}{\\text{DF}(t)}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $t$ = term, $d$ = document, $N$ = total documents, $\\text{DF}(t)$ = documents containing term $t$\n",
        "\n",
        "We use **cosine similarity** to compare query and document vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Retrieval\n",
        "print(\"Building TF-IDF model...\")\n",
        "\n",
        "# Initialize vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=100000)\n",
        "\n",
        "# Fit on documents\n",
        "tfidf_doc_matrix = tfidf_vectorizer.fit_transform(df_docs['content_clean'])\n",
        "print(f\"TF-IDF Document Matrix Shape: {tfidf_doc_matrix.shape}\")\n",
        "\n",
        "# Transform queries\n",
        "tfidf_query_matrix = tfidf_vectorizer.transform(df_queries_train['content_clean'])\n",
        "print(f\"TF-IDF Query Matrix Shape: {tfidf_query_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_topk_tfidf(query_matrix, doc_matrix, doc_ids, k=10):\n",
        "    \"\"\"Retrieve top-k documents for each query using TF-IDF + cosine similarity.\"\"\"\n",
        "    topk_indices = []\n",
        "    topk_scores = []\n",
        "    \n",
        "    # Compute similarities\n",
        "    similarities = cosine_similarity(query_matrix, doc_matrix)\n",
        "    \n",
        "    for i in tqdm(range(similarities.shape[0]), desc=\"TF-IDF Retrieval\"):\n",
        "        scores = similarities[i]\n",
        "        top_idx = np.argsort(scores)[-k:][::-1]\n",
        "        \n",
        "        topk_indices.append([doc_ids[j] for j in top_idx])\n",
        "        topk_scores.append(scores[top_idx].tolist())\n",
        "    \n",
        "    return topk_indices, topk_scores\n",
        "\n",
        "# Retrieve\n",
        "K = 10\n",
        "doc_ids = df_docs['id'].tolist()\n",
        "\n",
        "topk_indices_tfidf, topk_scores_tfidf = retrieve_topk_tfidf(\n",
        "    tfidf_query_matrix, tfidf_doc_matrix, doc_ids, k=K\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ TF-IDF retrieval complete!\")\n",
        "print(f\"Sample result for query 0: {topk_indices_tfidf[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 BM25+ Retrieval\n",
        "\n",
        "**How BM25+ works:**\n",
        "\n",
        "BM25+ is a ranking function used by search engines. It improves on TF-IDF by:\n",
        "- Adding **saturation**: Term frequency impact diminishes for very frequent terms\n",
        "- **Document length normalization**: Prevents bias towards longer documents\n",
        "- **+** variant adds a small constant to prevent zero scores\n",
        "\n",
        "$$\\text{BM25+}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\cdot \\frac{f(t, d) \\cdot (k_1 + 1)}{f(t, d) + k_1 \\cdot (1 - b + b \\cdot \\frac{|d|}{\\text{avgdl}})} + \\delta$$\n",
        "\n",
        "Parameters:\n",
        "- $k_1$ (typically 1.2-2.0): Controls term frequency saturation\n",
        "- $b$ (typically 0.75): Controls document length normalization\n",
        "- $\\delta$ (typically 1): Lower bound for term weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BM25+ Retrieval\n",
        "print(\"Building BM25+ model...\")\n",
        "\n",
        "# Tokenize corpus\n",
        "tokenized_corpus = [doc.split() for doc in df_docs['content_clean']]\n",
        "\n",
        "# Initialize BM25+\n",
        "bm25_model = BM25Plus(tokenized_corpus)\n",
        "\n",
        "print(f\"‚úÖ BM25+ model built on {len(tokenized_corpus):,} documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_topk_bm25(queries, bm25, doc_ids, k=10):\n",
        "    \"\"\"Retrieve top-k documents for each query using BM25+.\"\"\"\n",
        "    topk_indices = []\n",
        "    topk_scores = []\n",
        "    \n",
        "    for query in tqdm(queries, desc=\"BM25+ Retrieval\"):\n",
        "        tokenized_query = query.split()\n",
        "        scores = bm25.get_scores(tokenized_query)\n",
        "        \n",
        "        top_idx = np.argsort(scores)[-k:][::-1]\n",
        "        \n",
        "        topk_indices.append([doc_ids[j] for j in top_idx])\n",
        "        topk_scores.append(scores[top_idx].tolist())\n",
        "    \n",
        "    return topk_indices, topk_scores\n",
        "\n",
        "# Retrieve\n",
        "topk_indices_bm25, topk_scores_bm25 = retrieve_topk_bm25(\n",
        "    df_queries_train['content_clean'].tolist(), bm25_model, doc_ids, k=K\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ BM25+ retrieval complete!\")\n",
        "print(f\"Sample result for query 0: {topk_indices_bm25[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Text Embeddings (SentenceTransformers)\n",
        "\n",
        "**What are embeddings and why use them?**\n",
        "\n",
        "Text embeddings are dense vector representations that capture the **semantic meaning** of text. Unlike TF-IDF/BM25 which rely on exact word matching, embeddings can:\n",
        "\n",
        "1. **Understand synonyms**: \"car\" and \"automobile\" have similar embeddings\n",
        "2. **Capture context**: \"bank\" (financial) vs \"bank\" (river) get different representations\n",
        "3. **Handle paraphrases**: Different phrasings of the same idea are close in vector space\n",
        "\n",
        "We use **SentenceTransformers** which are pre-trained models specifically designed for semantic similarity tasks.\n",
        "\n",
        "**Model**: `all-MiniLM-L6-v2` - A good balance of speed and quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load SentenceTransformer model\n",
        "print(\"Loading SentenceTransformer model...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(f\"‚úÖ Model loaded!\")\n",
        "print(f\"   Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate document embeddings (this may take a while for large datasets)\n",
        "print(\"Generating document embeddings...\")\n",
        "print(\"‚ö†Ô∏è This may take several minutes for large datasets. Consider using GPU.\")\n",
        "\n",
        "# Use original content (not cleaned) for better semantic understanding\n",
        "doc_embeddings = embedding_model.encode(\n",
        "    df_docs['content'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Document embeddings shape: {doc_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate query embeddings\n",
        "print(\"Generating query embeddings...\")\n",
        "\n",
        "query_embeddings_train = embedding_model.encode(\n",
        "    df_queries_train['content'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Query embeddings shape: {query_embeddings_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_topk_embeddings(query_emb, doc_emb, doc_ids, k=10):\n",
        "    \"\"\"Retrieve top-k documents using embedding similarity.\"\"\"\n",
        "    topk_indices = []\n",
        "    topk_scores = []\n",
        "    \n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity(query_emb, doc_emb)\n",
        "    \n",
        "    for i in tqdm(range(len(query_emb)), desc=\"Embedding Retrieval\"):\n",
        "        scores = similarities[i]\n",
        "        top_idx = np.argsort(scores)[-k:][::-1]\n",
        "        \n",
        "        topk_indices.append([doc_ids[j] for j in top_idx])\n",
        "        topk_scores.append(scores[top_idx].tolist())\n",
        "    \n",
        "    return topk_indices, topk_scores\n",
        "\n",
        "# Retrieve\n",
        "topk_indices_emb, topk_scores_emb = retrieve_topk_embeddings(\n",
        "    query_embeddings_train, doc_embeddings, doc_ids, k=K\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Embedding retrieval complete!\")\n",
        "print(f\"Sample result for query 0: {topk_indices_emb[0][:3]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Visualization\n",
        "\n",
        "We use dimensionality reduction (t-SNE or UMAP) to visualize our embeddings in 2D and analyze category clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample embeddings for visualization (full dataset would be too slow)\n",
        "SAMPLE_SIZE = 5000\n",
        "\n",
        "# Random sample\n",
        "np.random.seed(42)\n",
        "sample_idx = np.random.choice(len(doc_embeddings), min(SAMPLE_SIZE, len(doc_embeddings)), replace=False)\n",
        "\n",
        "sample_embeddings = doc_embeddings[sample_idx]\n",
        "sample_categories = df_docs.iloc[sample_idx]['category'].values if 'category' in df_docs.columns else None\n",
        "\n",
        "print(f\"Sampled {len(sample_embeddings)} documents for visualization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-SNE visualization\n",
        "print(\"Running t-SNE dimensionality reduction...\")\n",
        "print(\"‚ö†Ô∏è This may take a few minutes...\")\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "embeddings_2d_tsne = tsne.fit_transform(sample_embeddings)\n",
        "\n",
        "print(f\"‚úÖ t-SNE complete! Output shape: {embeddings_2d_tsne.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot t-SNE results\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "if sample_categories is not None:\n",
        "    # Color by category\n",
        "    unique_cats = np.unique(sample_categories)\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_cats)))\n",
        "    \n",
        "    for i, cat in enumerate(unique_cats):\n",
        "        mask = sample_categories == cat\n",
        "        ax.scatter(\n",
        "            embeddings_2d_tsne[mask, 0],\n",
        "            embeddings_2d_tsne[mask, 1],\n",
        "            c=[colors[i]],\n",
        "            label=cat,\n",
        "            alpha=0.6,\n",
        "            s=10\n",
        "        )\n",
        "    ax.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "else:\n",
        "    ax.scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1], alpha=0.6, s=10)\n",
        "\n",
        "ax.set_title('t-SNE Visualization of Document Embeddings')\n",
        "ax.set_xlabel('t-SNE Dimension 1')\n",
        "ax.set_ylabel('t-SNE Dimension 2')\n",
        "plt.tight_layout()\n",
        "plt.savefig('reports/tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UMAP visualization (if available - often faster and better for large datasets)\n",
        "if UMAP_AVAILABLE:\n",
        "    print(\"Running UMAP dimensionality reduction...\")\n",
        "    \n",
        "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "    embeddings_2d_umap = reducer.fit_transform(sample_embeddings)\n",
        "    \n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    \n",
        "    if sample_categories is not None:\n",
        "        for i, cat in enumerate(unique_cats):\n",
        "            mask = sample_categories == cat\n",
        "            ax.scatter(\n",
        "                embeddings_2d_umap[mask, 0],\n",
        "                embeddings_2d_umap[mask, 1],\n",
        "                c=[colors[i]],\n",
        "                label=cat,\n",
        "                alpha=0.6,\n",
        "                s=10\n",
        "            )\n",
        "        ax.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    else:\n",
        "        ax.scatter(embeddings_2d_umap[:, 0], embeddings_2d_umap[:, 1], alpha=0.6, s=10)\n",
        "    \n",
        "    ax.set_title('UMAP Visualization of Document Embeddings')\n",
        "    ax.set_xlabel('UMAP Dimension 1')\n",
        "    ax.set_ylabel('UMAP Dimension 2')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('reports/umap_visualization.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"UMAP not available. Install with: pip install umap-learn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualization Analysis\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "*[Answer the following based on your visualizations:]*\n",
        "\n",
        "1. **Do you notice any clustering?** \n",
        "   - *TODO: Describe whether documents form distinct clusters*\n",
        "\n",
        "2. **Is there separation by category?**\n",
        "   - *TODO: Describe if different categories occupy different regions*\n",
        "\n",
        "3. **Why might this happen?**\n",
        "   - *TODO: Explain based on how embeddings capture semantic meaning*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Evaluation Suite\n",
        "\n",
        "We evaluate our retrieval systems using three standard metrics:\n",
        "- **Recall**: Did we find the relevant documents?\n",
        "- **Precision**: Are the retrieved documents relevant?\n",
        "- **MRR (Mean Reciprocal Rank)**: How early do relevant documents appear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_recall(retrieved_ids, relevant_ids):\n",
        "    \"\"\"\n",
        "    Compute Recall@k.\n",
        "    \n",
        "    Recall = # of relevant documents retrieved / # of total relevant documents\n",
        "    \n",
        "    Answers: Did the system find the documents that matter?\n",
        "    \"\"\"\n",
        "    if len(relevant_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    retrieved_set = set(retrieved_ids)\n",
        "    relevant_set = set(relevant_ids)\n",
        "    \n",
        "    return len(retrieved_set & relevant_set) / len(relevant_set)\n",
        "\n",
        "\n",
        "def compute_precision(retrieved_ids, relevant_ids):\n",
        "    \"\"\"\n",
        "    Compute Precision@k.\n",
        "    \n",
        "    Precision = # of relevant documents retrieved / # of total retrieved documents\n",
        "    \n",
        "    Answers: Are the top results useful?\n",
        "    \"\"\"\n",
        "    if len(retrieved_ids) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    retrieved_set = set(retrieved_ids)\n",
        "    relevant_set = set(relevant_ids)\n",
        "    \n",
        "    return len(retrieved_set & relevant_set) / len(retrieved_ids)\n",
        "\n",
        "\n",
        "def compute_mrr(retrieved_ids, relevant_ids):\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    MRR = 1 / rank of first relevant document\n",
        "    \n",
        "    Answers: How early does a relevant document appear?\n",
        "    \"\"\"\n",
        "    relevant_set = set(relevant_ids)\n",
        "    \n",
        "    for rank, doc_id in enumerate(retrieved_ids, start=1):\n",
        "        if doc_id in relevant_set:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0  # No relevant document found\n",
        "\n",
        "\n",
        "print(\"‚úÖ Evaluation functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_retrieval(topk_indices, ground_truth, query_ids):\n",
        "    \"\"\"\n",
        "    Evaluate a retrieval method across all queries.\n",
        "    \n",
        "    Returns:\n",
        "        dict with average recall, precision, and MRR\n",
        "    \"\"\"\n",
        "    recalls = []\n",
        "    precisions = []\n",
        "    mrrs = []\n",
        "    \n",
        "    for i, query_id in enumerate(query_ids):\n",
        "        retrieved = topk_indices[i]\n",
        "        relevant = ground_truth.get(query_id, [])\n",
        "        \n",
        "        recalls.append(compute_recall(retrieved, relevant))\n",
        "        precisions.append(compute_precision(retrieved, relevant))\n",
        "        mrrs.append(compute_mrr(retrieved, relevant))\n",
        "    \n",
        "    return {\n",
        "        'Recall@k': np.mean(recalls),\n",
        "        'Precision@k': np.mean(precisions),\n",
        "        'MRR': np.mean(mrrs),\n",
        "        'recalls': recalls,\n",
        "        'precisions': precisions,\n",
        "        'mrrs': mrrs\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Evaluation framework ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all methods\n",
        "query_ids = df_queries_train['id'].tolist()\n",
        "\n",
        "print(f\"Evaluating retrieval methods (k={K})...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# TF-IDF\n",
        "results_tfidf = evaluate_retrieval(topk_indices_tfidf, ground_truth, query_ids)\n",
        "print(f\"\\nüìä TF-IDF Results:\")\n",
        "print(f\"   Recall@{K}: {results_tfidf['Recall@k']:.4f}\")\n",
        "print(f\"   Precision@{K}: {results_tfidf['Precision@k']:.4f}\")\n",
        "print(f\"   MRR: {results_tfidf['MRR']:.4f}\")\n",
        "\n",
        "# BM25+\n",
        "results_bm25 = evaluate_retrieval(topk_indices_bm25, ground_truth, query_ids)\n",
        "print(f\"\\nüìä BM25+ Results:\")\n",
        "print(f\"   Recall@{K}: {results_bm25['Recall@k']:.4f}\")\n",
        "print(f\"   Precision@{K}: {results_bm25['Precision@k']:.4f}\")\n",
        "print(f\"   MRR: {results_bm25['MRR']:.4f}\")\n",
        "\n",
        "# Embeddings\n",
        "results_emb = evaluate_retrieval(topk_indices_emb, ground_truth, query_ids)\n",
        "print(f\"\\nüìä Embeddings Results:\")\n",
        "print(f\"   Recall@{K}: {results_emb['Recall@k']:.4f}\")\n",
        "print(f\"   Precision@{K}: {results_emb['Precision@k']:.4f}\")\n",
        "print(f\"   MRR: {results_emb['MRR']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Method': ['TF-IDF', 'BM25+', 'Embeddings'],\n",
        "    f'Recall@{K}': [\n",
        "        results_tfidf['Recall@k'],\n",
        "        results_bm25['Recall@k'],\n",
        "        results_emb['Recall@k']\n",
        "    ],\n",
        "    f'Precision@{K}': [\n",
        "        results_tfidf['Precision@k'],\n",
        "        results_bm25['Precision@k'],\n",
        "        results_emb['Precision@k']\n",
        "    ],\n",
        "    'MRR': [\n",
        "        results_tfidf['MRR'],\n",
        "        results_bm25['MRR'],\n",
        "        results_emb['MRR']\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "display(comparison_df)\n",
        "\n",
        "# Save results\n",
        "comparison_df.to_csv('reports/evaluation_results.csv', index=False)\n",
        "print(\"\\n‚úÖ Results saved to reports/evaluation_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = [f'Recall@{K}', f'Precision@{K}', 'MRR']\n",
        "colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
        "\n",
        "for ax, metric, color in zip(axes, metrics, colors):\n",
        "    values = comparison_df[metric].values\n",
        "    bars = ax.bar(comparison_df['Method'], values, color=color, alpha=0.8)\n",
        "    \n",
        "    # Add value labels\n",
        "    for bar, val in zip(bars, values):\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=11)\n",
        "    \n",
        "    ax.set_title(metric, fontsize=14, fontweight='bold')\n",
        "    ax.set_ylim(0, max(values) * 1.2)\n",
        "    ax.set_ylabel('Score')\n",
        "\n",
        "plt.suptitle('Retrieval Methods Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('reports/model_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Analysis\n",
        "\n",
        "**Questions to answer:**\n",
        "\n",
        "1. **Which model performs best overall?**\n",
        "   - *TODO: Compare and discuss*\n",
        "\n",
        "2. **Why might one model outperform others?**\n",
        "   - *TODO: Discuss strengths/weaknesses of each approach*\n",
        "\n",
        "3. **What are the trade-offs?**\n",
        "   - *TODO: Consider speed, accuracy, and resource requirements*\n",
        "\n",
        "4. **How does changing k affect results?**\n",
        "   - *TODO: Experiment and discuss*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Kaggle Submission\n",
        "\n",
        "Generate predictions for the test queries and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions for test queries using best method\n",
        "# (Choose based on evaluation results)\n",
        "\n",
        "print(\"Generating predictions for test queries...\")\n",
        "\n",
        "# Transform test queries\n",
        "test_query_matrix = tfidf_vectorizer.transform(df_queries_test['content_clean'])\n",
        "\n",
        "# Retrieve (using TF-IDF as example - change to best method)\n",
        "test_topk_indices, test_topk_scores = retrieve_topk_tfidf(\n",
        "    test_query_matrix, tfidf_doc_matrix, doc_ids, k=K\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Generated predictions for {len(test_topk_indices)} test queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create submission file\n",
        "submission = {\n",
        "    query_id: retrieved\n",
        "    for query_id, retrieved in zip(df_queries_test['id'].tolist(), test_topk_indices)\n",
        "}\n",
        "\n",
        "# Save\n",
        "with open('submission_phase1.json', 'w') as f:\n",
        "    json.dump(submission, f)\n",
        "\n",
        "print(\"‚úÖ Submission file created: submission_phase1.json\")\n",
        "print(f\"   Total queries: {len(submission)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all results for later use\n",
        "results = {\n",
        "    'topk_indices_tfidf': topk_indices_tfidf,\n",
        "    'topk_scores_tfidf': topk_scores_tfidf,\n",
        "    'topk_indices_bm25': topk_indices_bm25,\n",
        "    'topk_scores_bm25': topk_scores_bm25,\n",
        "    'topk_indices_emb': topk_indices_emb,\n",
        "    'topk_scores_emb': topk_scores_emb,\n",
        "    'evaluation_results': {\n",
        "        'tfidf': results_tfidf,\n",
        "        'bm25': results_bm25,\n",
        "        'embeddings': results_emb\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('all_results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(\"‚úÖ All results saved to all_results.pkl\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
