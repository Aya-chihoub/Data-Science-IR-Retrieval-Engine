{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Saved Data & Quick Start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading Saved saved results...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved pickle file\n",
    "try:\n",
    "    with open('results.pkl', 'rb') as f:\n",
    "        nour_data = pickle.load(f)\n",
    "    \n",
    "    print(\"‚úÖ Successfully loaded results.pkl\")\n",
    "    print(f\"\\nType: {type(nour_data)}\")\n",
    "    \n",
    "    if isinstance(nour_data, dict):\n",
    "        print(f\"\\nüì¶ Contents ({len(nour_data)} keys):\")\n",
    "        for key, value in nour_data.items():\n",
    "            vtype = type(value).__name__\n",
    "            if hasattr(value, 'shape'):\n",
    "                print(f\"   {key}: {vtype} with shape {value.shape}\")\n",
    "            elif hasattr(value, '__len__'):\n",
    "                print(f\"   {key}: {vtype} with length {len(value)}\")\n",
    "            else:\n",
    "                print(f\"   {key}: {vtype}\")\n",
    "    elif isinstance(nour_data, (list, tuple)):\n",
    "        print(f\"\\nList/tuple with {len(nour_data)} items\")\n",
    "        print(f\"First item type: {type(nour_data[0])}\")\n",
    "    elif isinstance(nour_data, pd.DataFrame):\n",
    "        print(f\"\\nDataFrame with shape: {nour_data.shape}\")\n",
    "        print(f\"Columns: {list(nour_data.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pickle: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to peek at specific data\n",
    "if isinstance(nour_data, dict):\n",
    "    for key in list(nour_data.keys())[:3]:  # First 3 keys\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Key: {key}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        val = nour_data[key]\n",
    "        if isinstance(val, (list, np.ndarray)):\n",
    "            print(f\"First few items: {val[:3] if len(val) > 3 else val}\")\n",
    "        elif isinstance(val, pd.DataFrame):\n",
    "            display(val.head(2))\n",
    "        else:\n",
    "            print(f\"Value: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check the XLS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to peek at the XLS file (might be large)\n",
    "import os\n",
    "\n",
    "xls_path = 'docs_with_content.xls'\n",
    "if os.path.exists(xls_path):\n",
    "    size_mb = os.path.getsize(xls_path) / (1024*1024)\n",
    "    print(f\"üìÅ File size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    print(\"\\n‚è≥ Attempting to read first few rows (this may take a moment)...\")\n",
    "    try:\n",
    "        # Try reading as CSV (sometimes xls is actually CSV)\n",
    "        df_sample = pd.read_csv(xls_path, nrows=5)\n",
    "        print(f\"\\n‚úÖ Loaded as CSV!\")\n",
    "        print(f\"Columns: {list(df_sample.columns)}\")\n",
    "        display(df_sample)\n",
    "    except:\n",
    "        try:\n",
    "            # Try as actual Excel\n",
    "            df_sample = pd.read_excel(xls_path, nrows=5)\n",
    "            print(f\"\\n‚úÖ Loaded as Excel!\")\n",
    "            print(f\"Columns: {list(df_sample.columns)}\")\n",
    "            display(df_sample)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Could not read: {e}\")\n",
    "else:\n",
    "    print(\"XLS file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Data Instructions\n",
    "\n",
    "To run the full project, download the dataset from Kaggle:\n",
    "\n",
    "1. **Go to**: https://www.kaggle.com/t/f383bc6c1f194226bb43d21ab3d65418\n",
    "2. **Download** these files:\n",
    "   - `docs.json` - Document collection\n",
    "   - `queries_train.json` - Training queries\n",
    "   - `queries_test.json` - Test queries\n",
    "   - `qgts_train.json` - Ground truth\n",
    "3. **Place** all files in the `data/` folder\n",
    "4. **Run** the main notebook: `IR_Project_Phase1.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
